# Káº¿ Hoáº¡ch Chi Tiáº¿t: Pipeline Äá»•i MÃ u TÆ°á»ng vá»›i Diffusion

> **NgÃ y táº¡o:** 2026-02-07  
> **Má»¥c tiÃªu:** XÃ¢y dá»±ng pipeline training vÃ  inference hoÃ n chá»‰nh cho bÃ i toÃ¡n Ä‘á»•i mÃ u tÆ°á»ng dá»±a trÃªn diffusion, tham kháº£o cÃ¡c paper ná»•i tiáº¿ng nhÆ° Vi-TryOn, RoomEditor

---

## ğŸ“‹ Má»¥c Lá»¥c

1. [PhÃ¢n TÃ­ch Hiá»‡n Tráº¡ng](#1-phÃ¢n-tÃ­ch-hiá»‡n-tráº¡ng)
2. [Kiáº¿n TrÃºc Äá» Xuáº¥t](#2-kiáº¿n-trÃºc-Ä‘á»-xuáº¥t)
3. [Pipeline Training](#3-pipeline-training)
4. [Pipeline Inference](#4-pipeline-inference)
5. [Káº¿ Hoáº¡ch Triá»ƒn Khai](#5-káº¿-hoáº¡ch-triá»ƒn-khai)
6. [CÃ¡c Váº¥n Äá» Cáº§n Giáº£i Quyáº¿t](#6-cÃ¡c-váº¥n-Ä‘á»-cáº§n-giáº£i-quyáº¿t)

---

## 1. PhÃ¢n TÃ­ch Hiá»‡n Tráº¡ng

### 1.1 Cáº¥u TrÃºc Dataset Hiá»‡n Táº¡i (`./dataset_test`)

```
dataset_test/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â”œâ”€â”€ ADE_frame_00000004_original.png    # áº¢nh tÆ°á»ng cÅ©
â”‚   â”‚   â”œâ”€â”€ ADE_frame_00000004_burgundy_0.png # áº¢nh tÆ°á»ng má»›i (GT)
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ masks/
â”‚   â”‚   â””â”€â”€ ADE_frame_00000004.png            # Mask vÃ¹ng tÆ°á»ng
â”‚   â””â”€â”€ metadata.jsonl
â””â”€â”€ validation/
    â””â”€â”€ ...
```

**Metadata format:**
```json
{
  "source_path": "train/images/ADE_frame_00000004_original.png",
  "target_path": "train/images/ADE_frame_00000004_burgundy_0.png",
  "mask_path": "train/masks/ADE_frame_00000004.png",
  "color_rgb": [128, 0, 32],
  "color_name": "burgundy"
}
```

### 1.2 Kiáº¿n TrÃºc Hiá»‡n Táº¡i

#### âœ… ÄÃ£ cÃ³:
- **Base Pipeline** (`pipeline.py`): SD Inpainting + ControlNet Depth + IP-Adapter Plus
- **Segmentation** (`segmentation.py`): SAM2/FastSAM Ä‘á»ƒ táº¡o mask
- **Dataset Preparation** (`prepare_dataset_v2.py`): Táº¡o training pairs tá»« source images
- **Training Script** (`train.py`): LoRA training vá»›i UNet
- **Inference Script** (`inference.py`): Inference vá»›i pipeline

#### âŒ Thiáº¿u/Cáº§n cáº£i thiá»‡n:
1. **Module `models/wall_recoloring_pipeline.py`**: ChÆ°a tá»“n táº¡i, nhÆ°ng Ä‘Æ°á»£c import trong `train.py` vÃ  `inference.py`
2. **Module `dataset/wall_paint_dataset.py`**: ChÆ°a tá»“n táº¡i, nhÆ°ng Ä‘Æ°á»£c import trong `train.py`
3. **Training Strategy**: Hiá»‡n táº¡i train Ä‘á»ƒ reconstruct original image, khÃ´ng phÃ¹ há»£p vá»›i má»¥c tiÃªu Ä‘á»•i mÃ u
4. **Condition Integration**: CÃ¡ch ghÃ©p ná»‘i cÃ¡c condition (source, mask, ref) chÆ°a tá»‘i Æ°u

### 1.3 Váº¥n Äá» ChÃ­nh

#### ğŸ”´ Váº¥n Äá» 1: Train/Inference Mismatch
```
TRAINING:
- Input: áº¢nh tÆ°á»ng cÅ© (noisy)
- Condition: Source image, mask, color ref
- Target: áº¢nh tÆ°á»ng cÅ© (original) âŒ SAI!

INFERENCE:
- Input: áº¢nh tÆ°á»ng cÅ©
- Condition: Source image, mask, color ref má»›i
- Target: áº¢nh tÆ°á»ng má»›i âœ… ÄÃšNG!

â†’ Model há»c reconstruct original, nhÆ°ng inference muá»‘n Ä‘á»•i mÃ u!
```

#### ğŸ”´ Váº¥n Äá» 2: Condition Integration ChÆ°a Tá»‘i Æ¯u
- ControlNet: DÃ¹ng Canny (trong `inference.py`) nhÆ°ng spec nÃ³i Depth
- IP-Adapter: Chá»‰ dÃ¹ng á»Ÿ inference, khÃ´ng Ä‘Æ°á»£c train cÃ¹ng LoRA
- Mask: ÄÆ°á»£c concatenate vÃ o UNet input, nhÆ°ng cÃ¡ch sá»­ dá»¥ng chÆ°a rÃµ rÃ ng

#### ğŸ”´ Váº¥n Äá» 3: Thiáº¿u Reference Image trong Training
- Dataset cÃ³ `color_rgb` nhÆ°ng khÃ´ng cÃ³ reference image
- Training khÃ´ng sá»­ dá»¥ng IP-Adapter image embeddings
- Model khÃ´ng há»c cÃ¡ch sá»­ dá»¥ng color reference

---

## 2. Kiáº¿n TrÃºc Äá» Xuáº¥t

### 2.1 Tham Kháº£o CÃ¡c Paper Ná»•i Tiáº¿ng

#### Vi-TryOn (Virtual Try-On)
```
Key Ideas:
1. Dual-branch architecture: Structure + Appearance
2. Warping module Ä‘á»ƒ preserve structure
3. Feature fusion á»Ÿ multiple scales
4. Reference image Ä‘Æ°á»£c encode vÃ  inject vÃ o UNet

Ãp dá»¥ng cho Wall Recoloring:
- Source image â†’ Structure branch (ControlNet)
- Color reference â†’ Appearance branch (IP-Adapter)
- Mask â†’ Guide inpainting region
```

#### RoomEditor
```
Key Ideas:
1. Multi-condition control: Layout + Style + Object
2. Hierarchical conditioning: Global â†’ Local
3. Attention-based feature fusion
4. Progressive refinement

Ãp dá»¥ng cho Wall Recoloring:
- Global: Depth map (ControlNet)
- Local: Color reference (IP-Adapter)
- Mask: Spatial guidance
```

### 2.2 Kiáº¿n TrÃºc Pipeline Äá» Xuáº¥t

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              WALL RECOLORING PIPELINE ARCHITECTURE              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  INPUTS:                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Source Image â”‚  â”‚   Mask   â”‚  â”‚ Color Ref    â”‚            â”‚
â”‚  â”‚ (Old Wall)   â”‚  â”‚          â”‚  â”‚ (New Color)  â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚         â”‚               â”‚               â”‚                      â”‚
â”‚         â–¼               â–¼               â–¼                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              PREPROCESSING STAGE                      â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â”‚  1. Source â†’ Depth Map (ControlNet Preprocessor)    â”‚    â”‚
â”‚  â”‚  2. Color Ref â†’ CLIP Embedding (IP-Adapter)          â”‚    â”‚
â”‚  â”‚  3. Mask â†’ Latent Mask (Resize to 64x64)            â”‚    â”‚
â”‚  â”‚  4. Source â†’ Masked Source (for inpainting)          â”‚    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚               â”‚               â”‚                      â”‚
â”‚         â–¼               â–¼               â–¼                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              CONDITIONING STAGE                       â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚  â”‚ ControlNet Branch (Structure Preservation) â”‚    â”‚    â”‚
â”‚  â”‚  â”‚ Input: Depth Map                            â”‚    â”‚    â”‚
â”‚  â”‚  â”‚ Output: Down/Mid Block Residuals           â”‚    â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚  â”‚ IP-Adapter Branch (Color Transfer)          â”‚    â”‚    â”‚
â”‚  â”‚  â”‚ Input: Color Reference Image                â”‚    â”‚    â”‚
â”‚  â”‚  â”‚ Output: Image Embeddings (added_cond_kwargs)â”‚    â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚  â”‚ Inpainting Branch (Masked Generation)        â”‚    â”‚    â”‚
â”‚  â”‚  â”‚ Input: Noisy Latents + Mask + Masked Source â”‚    â”‚    â”‚
â”‚  â”‚  â”‚ Output: Concatenated Input (9 channels)    â”‚    â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚               â”‚               â”‚                      â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                           â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              UNET GENERATION STAGE                   â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â”‚  UNet Input:                                          â”‚    â”‚
â”‚  â”‚  - Latent Model Input: [noisy_latents, mask, masked]â”‚    â”‚
â”‚  â”‚  - Text Embeddings: encoder_hidden_states            â”‚    â”‚
â”‚  â”‚  - ControlNet Residuals: down_block + mid_block     â”‚    â”‚
â”‚  â”‚  - IP-Adapter Embeddings: image_embeds              â”‚    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â”‚  UNet Output:                                         â”‚    â”‚
â”‚  â”‚  - Noise Prediction: ÎµÌ‚                               â”‚    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              POSTPROCESSING STAGE                    â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â”‚  1. Denoising Loop (DDIM/DPM++)                      â”‚    â”‚
â”‚  â”‚  2. VAE Decode: Latents â†’ Image                      â”‚    â”‚
â”‚  â”‚  3. Post-process: Resize, Blend edges               â”‚    â”‚
â”‚  â”‚                                                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â–¼                                    â”‚
â”‚                    OUTPUT IMAGE                                â”‚
â”‚                  (New Wall Color)                              â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.3 Chi Tiáº¿t CÃ¡c Component

#### A. ControlNet Branch
```python
# Má»¥c Ä‘Ã­ch: Preserve structure vÃ  depth information
ControlNet(
    input: depth_map,  # Tá»« source image
    condition: depth_map,
    timestep: t,
    encoder_hidden_states: text_embeds
) â†’ (down_block_residuals, mid_block_residual)
```

**Lá»±a chá»n ControlNet:**
- âœ… **Depth** (`control_v11f1p_sd15_depth`): Tá»‘t nháº¥t cho structure preservation
- âŒ **Canny**: Chá»‰ edge, khÃ´ng cÃ³ depth info
- âœ… **Canny + Depth**: CÃ³ thá»ƒ combine náº¿u cáº§n

#### B. IP-Adapter Branch
```python
# Má»¥c Ä‘Ã­ch: Transfer color/style tá»« reference
CLIPVisionEncoder(
    input: color_reference_image  # [224, 224, 3]
) â†’ image_embeds  # [B, 1024]

# Inject vÃ o UNet qua attention mechanism
UNet.forward(
    ...,
    added_cond_kwargs={"image_embeds": image_embeds}
)
```

**Lá»±a chá»n IP-Adapter:**
- âœ… **IP-Adapter Plus** (`ip-adapter-plus_sd15.bin`): Tá»‘t hÆ¡n cho color transfer
- âœ… **IP-Adapter Full**: Náº¿u cáº§n fine-grained control

#### C. Inpainting Branch
```python
# Má»¥c Ä‘Ã­ch: Guide generation trong masked region
VAE.encode(masked_source) â†’ masked_latents  # [B, 4, 64, 64]
F.interpolate(mask) â†’ mask_latents          # [B, 1, 64, 64]

# Concatenate vá»›i noisy latents
unet_input = torch.cat([
    noisy_latents,      # [B, 4, 64, 64]
    mask_latents,        # [B, 1, 64, 64]
    masked_latents       # [B, 4, 64, 64]
], dim=1)  # â†’ [B, 9, 64, 64]
```

---

## 3. Pipeline Training

### 3.1 Training Strategy Äá» Xuáº¥t

#### âŒ Strategy CÅ© (SAI):
```
Input: áº¢nh tÆ°á»ng cÅ© (noisy)
Target: áº¢nh tÆ°á»ng cÅ© (original)
â†’ Model há»c reconstruct original
```

#### âœ… Strategy Má»›i (ÄÃšNG):
```
Input: áº¢nh tÆ°á»ng má»›i (noisy) â† ÄÃ¢y lÃ  key!
Condition: 
  - Source: áº¢nh tÆ°á»ng cÅ© (cho ControlNet)
  - Mask: VÃ¹ng tÆ°á»ng
  - Color Ref: MÃ u má»›i (cho IP-Adapter)
Target: áº¢nh tÆ°á»ng má»›i (GT)
â†’ Model há»c generate new color tá»« conditions
```

### 3.2 Training Flow Chi Tiáº¿t

```python
def training_step(batch):
    # 1. Load data
    source_image = batch["source"]      # áº¢nh tÆ°á»ng cÅ©
    target_image = batch["target"]      # áº¢nh tÆ°á»ng má»›i (GT)
    mask = batch["mask"]
    color_ref = batch["color_reference"]  # Color patch má»›i
    
    # 2. Encode target (GT) to latent
    target_latents = vae.encode(target_image).latent_dist.sample()
    target_latents = target_latents * vae.config.scaling_factor
    
    # 3. Add noise to target (NOT source!)
    noise = torch.randn_like(target_latents)
    timesteps = torch.randint(0, 1000, (bsz,))
    noisy_latents = scheduler.add_noise(target_latents, noise, timesteps)
    
    # 4. Prepare conditions
    # A. ControlNet: Use SOURCE image (old wall)
    depth_map = depth_estimator(source_image)
    controlnet_output = controlnet(
        noisy_latents,
        timesteps,
        encoder_hidden_states=text_embeds,
        controlnet_cond=depth_map
    )
    
    # B. IP-Adapter: Use COLOR REFERENCE (new color)
    color_ref_224 = F.interpolate(color_ref, (224, 224))
    color_ref_normalized = normalize_clip(color_ref_224)
    image_embeds = image_encoder(color_ref_normalized).image_embeds
    added_cond_kwargs = {"image_embeds": image_embeds}
    
    # C. Inpainting: Prepare masked source
    masked_source = source_image * (1 - mask)  # Use SOURCE, not target!
    masked_source_latents = vae.encode(masked_source).latent_dist.sample()
    masked_source_latents = masked_source_latents * vae.config.scaling_factor
    mask_latents = F.interpolate(mask, size=(64, 64), mode="nearest")
    
    # 5. UNet forward
    unet_input = torch.cat([
        noisy_latents,      # Noisy TARGET latents
        mask_latents,
        masked_source_latents  # Masked SOURCE latents
    ], dim=1)
    
    noise_pred = unet(
        unet_input,
        timesteps,
        encoder_hidden_states=text_embeds,
        down_block_additional_residuals=controlnet_output[0],
        mid_block_additional_residual=controlnet_output[1],
        added_cond_kwargs=added_cond_kwargs
    ).sample
    
    # 6. Loss: Predict noise added to TARGET
    loss = F.mse_loss(noise_pred, noise)
    return loss
```

### 3.3 Dataset Format Cho Training

```python
class WallPaintDataset(Dataset):
    """
    Dataset cho training wall recoloring.
    
    Expected structure:
    - source_path: áº¢nh tÆ°á»ng cÅ©
    - target_path: áº¢nh tÆ°á»ng má»›i (GT)
    - mask_path: Mask vÃ¹ng tÆ°á»ng
    - color_reference: Color patch má»›i (táº¡o tá»« color_rgb)
    """
    
    def __getitem__(self, idx):
        # Load images
        source = load_image(self.samples[idx]["source_path"])
        target = load_image(self.samples[idx]["target_path"])
        mask = load_mask(self.samples[idx]["mask_path"])
        
        # Create color reference patch
        color_rgb = self.samples[idx]["color_rgb"]
        color_ref = create_color_patch(color_rgb, size=512)
        
        # Augmentations
        source, target, mask, color_ref = self.transform(
            source, target, mask, color_ref
        )
        
        return {
            "source": source,           # [3, 512, 512]
            "target": target,           # [3, 512, 512] - GT
            "mask": mask,               # [1, 512, 512]
            "color_reference": color_ref,  # [3, 512, 512]
            "prompt": "interior wall, high quality"
        }
```

### 3.4 LoRA Training Configuration

```python
# LoRA config cho UNet
lora_config = LoraConfig(
    r=16,                    # Rank (tÄƒng tá»« 8 â†’ 16 cho color task)
    lora_alpha=32,           # Alpha = 2 * r
    target_modules=[
        "to_k", "to_q", "to_v", "to_out.0",  # Attention layers
        # CÃ³ thá»ƒ thÃªm:
        # "ff.net.0.proj", "ff.net.2"  # FFN layers (náº¿u cáº§n)
    ],
    lora_dropout=0.0,
    bias="none",
)

# Freeze cÃ¡c components khÃ¡c
vae.requires_grad_(False)
text_encoder.requires_grad_(False)
controlnet.requires_grad_(False)
image_encoder.requires_grad_(False)  # IP-Adapter encoder

# Chá»‰ train UNet LoRA
unet.requires_grad_(False)
unet = get_peft_model(unet, lora_config)
```

---

## 4. Pipeline Inference

### 4.1 Inference Flow

```python
def inference(source_image, mask, color_reference):
    # 1. Preprocess
    source_512 = resize(source_image, (512, 512))
    mask_512 = resize(mask, (512, 512))
    color_ref_224 = resize(color_reference, (224, 224))
    
    # 2. Prepare conditions
    depth_map = depth_estimator(source_512)
    color_embeds = image_encoder(normalize_clip(color_ref_224)).image_embeds
    
    # 3. Initialize latents (random noise)
    latents = torch.randn((1, 4, 64, 64))
    
    # 4. Prepare inpainting inputs
    masked_source = source_512 * (1 - mask_512)
    masked_latents = vae.encode(masked_source).latent_dist.sample()
    mask_latents = F.interpolate(mask_512, (64, 64), mode="nearest")
    
    # 5. Denoising loop
    scheduler.set_timesteps(50)
    for t in scheduler.timesteps:
        # ControlNet
        controlnet_output = controlnet(
            latents, t, text_embeds, depth_map
        )
        
        # UNet input
        unet_input = torch.cat([
            latents,
            mask_latents,
            masked_latents
        ], dim=1)
        
        # UNet prediction
        noise_pred = unet(
            unet_input,
            t,
            encoder_hidden_states=text_embeds,
            down_block_additional_residuals=controlnet_output[0],
            mid_block_additional_residual=controlnet_output[1],
            added_cond_kwargs={"image_embeds": color_embeds}
        ).sample
        
        # Step
        latents = scheduler.step(noise_pred, t, latents).prev_sample
    
    # 6. Decode
    image = vae.decode(latents / vae.config.scaling_factor).sample
    image = postprocess(image)
    
    return image
```

### 4.2 Hyperparameters Inference

```python
INFERENCE_CONFIG = {
    "num_inference_steps": 50,           # DDIM steps
    "guidance_scale": 5.0,                # CFG scale
    "controlnet_conditioning_scale": 0.8, # ControlNet strength
    "ip_adapter_scale": 0.7,              # IP-Adapter strength
    "strength": 1.0,                      # Denoising strength
}
```

---

## 5. Káº¿ Hoáº¡ch Triá»ƒn Khai

### Phase 1: Táº¡o Missing Modules (Æ¯u tiÃªn cao)

#### 5.1 Táº¡o `models/wall_recoloring_pipeline.py`
```python
# File nÃ y sáº½ wrap pipeline hiá»‡n táº¡i vÃ  thÃªm cÃ¡c utilities
def get_wall_recoloring_pipeline(
    base_model_path,
    controlnet_path,
    ip_adapter_scale=0.7,
    device="cuda"
):
    # Load vÃ  combine cÃ¡c components
    # Return pipeline ready to use
```

#### 5.2 Táº¡o `dataset/wall_paint_dataset.py`
```python
# File nÃ y sáº½ implement WallPaintDataset
class WallPaintDataset:
    # Load tá»« metadata.jsonl
    # Return source, target, mask, color_reference
```

### Phase 2: Fix Training Strategy

#### 5.3 Sá»­a `train.py`
- [ ] Thay Ä‘á»•i target tá»« source â†’ target image
- [ ] ThÃªm color reference vÃ o training loop
- [ ] Fix ControlNet Ä‘á»ƒ dÃ¹ng Depth thay vÃ¬ Canny
- [ ] Äáº£m báº£o IP-Adapter Ä‘Æ°á»£c sá»­ dá»¥ng trong training

### Phase 3: Cáº£i Thiá»‡n Dataset

#### 5.4 Cáº£i thiá»‡n `prepare_dataset_v2.py`
- [ ] Äáº£m báº£o táº¡o color reference patches
- [ ] Validate dataset format
- [ ] Add data augmentation

### Phase 4: Testing & Validation

#### 5.5 Táº¡o validation script
- [ ] Visual validation trong training
- [ ] Quantitative metrics (color accuracy, structure preservation)
- [ ] A/B testing vá»›i different strategies

---

## 6. CÃ¡c Váº¥n Äá» Cáº§n Giáº£i Quyáº¿t

### 6.1 Train/Inference Alignment âœ…
**Giáº£i phÃ¡p:** Train vá»›i target image lÃ m noisy input, source image lÃ m condition

### 6.2 Color Reference Generation
**Váº¥n Ä‘á»:** Dataset chá»‰ cÃ³ `color_rgb`, cáº§n táº¡o color patch
**Giáº£i phÃ¡p:** 
```python
def create_color_patch(rgb, size=512):
    # Táº¡o solid color patch vá»›i texture nháº¹
    # CÃ³ thá»ƒ thÃªm gradient, noise Ä‘á»ƒ CLIP encode tá»‘t hÆ¡n
```

### 6.3 ControlNet Type
**Váº¥n Ä‘á»:** `inference.py` dÃ¹ng Canny, nhÆ°ng spec nÃ³i Depth
**Giáº£i phÃ¡p:** Standardize vá» Depth cho structure preservation tá»‘t hÆ¡n

### 6.4 IP-Adapter Training
**Váº¥n Ä‘á»:** IP-Adapter weights frozen, khÃ´ng Ä‘Æ°á»£c train
**Giáº£i phÃ¡p:** 
- Option 1: Giá»¯ frozen, chá»‰ train LoRA (Ä‘Æ¡n giáº£n hÆ¡n)
- Option 2: Fine-tune IP-Adapter (phá»©c táº¡p hÆ¡n, cáº§n nhiá»u data)

### 6.5 Mask Quality
**Váº¥n Ä‘á»:** Mask cÃ³ thá»ƒ cÃ³ noise, khÃ´ng chÃ­nh xÃ¡c
**Giáº£i phÃ¡p:** 
- Erosion/dilation Ä‘á»ƒ clean mask
- Validation mask quality trong dataset prep

---

## 7. Next Steps

### Immediate (Tuáº§n 1):
1. âœ… Táº¡o `models/wall_recoloring_pipeline.py`
2. âœ… Táº¡o `dataset/wall_paint_dataset.py`
3. âœ… Fix `train.py` vá»›i training strategy má»›i
4. âœ… Test training vá»›i dataset_test

### Short-term (Tuáº§n 2-3):
1. Cáº£i thiá»‡n dataset preparation
2. Add validation metrics
3. Hyperparameter tuning
4. Documentation

### Long-term (ThÃ¡ng 2+):
1. Scale up dataset
2. Advanced techniques (multi-scale, progressive refinement)
3. Production deployment

---

## 8. References

- **Vi-TryOn**: Virtual Try-On with Diffusion Models
- **RoomEditor**: Room Editing with Diffusion Models
- **Paint-by-Example**: Reference-based Inpainting
- **IP-Adapter**: Effective Image Adapter for Diffusion Models
- **ControlNet**: Adding Conditional Control to Diffusion Models

---

**TÃ¡c giáº£:** AI Assistant  
**NgÃ y cáº­p nháº­t:** 2026-02-07
